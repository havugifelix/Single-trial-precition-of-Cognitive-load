{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import hyperopt\n",
    "import os\n",
    "import seaborn as sn\n",
    "from hpsklearn import HyperoptEstimator,random_forest,HyperoptEstimator,any_classifier,any_preprocessing\n",
    "import hyperopt as hp\n",
    "from hpsklearn import svc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hyperopt import tpe\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/kashraf/Elements/Dissertation/data/preprocessed/visual/psd_data_noisy/\"\n",
    "data1 = pd.read_csv(os.path.join(path,\"beta_psd.csv\"),index_col=0).iloc[:,:64]\n",
    "data2 = pd.read_csv(os.path.join(path,\"theta_psd.csv\"),index_col=0).iloc[:,:64]\n",
    "data3 = pd.read_csv(os.path.join(path,\"alpha_psd.csv\"),index_col=0)\n",
    "\n",
    "data= pd.concat([data1,data2,data3],axis=1)\n",
    "\n",
    "x=data[data.columns[:64]]\n",
    "y=data.iloc[:,-1]\n",
    "\n",
    "scaler=StandardScaler()\n",
    "x=scaler.fit_transform(x)\n",
    "\n",
    "pca1=PCA()\n",
    "\n",
    "pca_data=pca1.fit_transform(x)\n",
    "components=pca1.components_\n",
    "explained_var=pca1.explained_variance_ratio_\n",
    "plt.style.use(\"seaborn\")\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(len(components)),np.cumsum(explained_var))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigen_sample_select(data,n_comps):\n",
    "    pca=PCA(n_components=n_comps)\n",
    "    \n",
    "    # Fit and project\n",
    "    projected=pca.fit_transform(data)\n",
    "    eigen_space=pca.components_\n",
    "    \n",
    "    # Renconstuction\n",
    "    recons=np.dot(projected,eigen_space)\n",
    "    recon_error=data-recons\n",
    "    recon_error=np.linalg.norm(recon_error,axis=1)\n",
    "    \n",
    "    return recon_error\n",
    "err=eigen_sample_select(x,40)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.hist(np.log(err))\n",
    "plt.xlabel(\"Log reconstruction error\"),plt.ylabel(\"Number of samples\")\n",
    "# plt.show()\n",
    "# plt.savefig(\"Errorhist.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci_data_selection(errors):\n",
    "    error=np.log(errors)\n",
    "    sorted_data=np.sort(error)\n",
    "    l_limit= np.percentile(sorted_data,25)\n",
    "    u_limit=np.percentile(sorted_data,75)\n",
    "    id_selected=[]\n",
    "    for index , err in enumerate(error):\n",
    "        if l_limit<=err and err<=u_limit:\n",
    "            id_selected.append(index)\n",
    "    return id_selected\n",
    "selected_index=ci_data_selection(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=pd.DataFrame(data.values[selected_index])\n",
    "x=new_data.iloc[:,:64]\n",
    "y=new_data.iloc[:,64]\n",
    "\n",
    "scaler=StandardScaler()\n",
    "x=scaler.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=PCA(n_components=75).fit_transform(x)\n",
    "x_train,x_test,y_train, y_test= train_test_split(x,y,test_size=0.3)\n",
    "print(\"X_train:\",x_train.shape)\n",
    "print(\"x_test:\",x_test.shape)\n",
    "print(\"y_train:\",y_train.shape)\n",
    "print(\"y_test:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estim=HyperoptEstimator(classifier=any_classifier(\"clf\"),algo=tpe.suggest,max_evals=100,\n",
    "                          trial_timeout=100,n_jobs=12)\n",
    "estim.fit(x_train,y_train)\n",
    "\n",
    "print(estim.best_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model= RandomForestClassifier(bootstrap=True, max_depth=4, max_features='sqrt',\n",
    "#                        min_samples_leaf=9, n_estimators=10, n_jobs=12,\n",
    "#                        random_state=0, verbose=False)\n",
    "model=AdaBoostClassifier(algorithm='SAMME', learning_rate=0.00043987561899226445,\n",
    "                   n_estimators=12, random_state=3)\n",
    "\n",
    "# model=SVC(C=10000, cache_size=512, degree=1, gamma='auto',\n",
    "#     kernel='rbf', max_iter=16883756.0, random_state=2,\n",
    "#     tol=0.0010625608463669858)\n",
    "x_train=MinMaxScaler().fit_transform(x_train)\n",
    "model.fit(x_train,y_train)\n",
    "x_test=MinMaxScaler().fit_transform(x_test)\n",
    "y_pred=model.predict(x_test)\n",
    "print(\"Report: \", classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "conf_mat=confusion_matrix(y_test,y_pred,normalize=\"true\")\n",
    "labels=[\"CL-1\",\"CL-2\",\"CL-3\",\"CL-4\"]\n",
    "heat=sn.heatmap(conf_mat,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                cmap=\"Blues\",\n",
    "                xticklabels=labels,\n",
    "               yticklabels=labels)\n",
    "# plt.title(titles[5])\n",
    "# plt.savefig(path+titles[5]+\".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
